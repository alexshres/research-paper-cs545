
\section{Abstract}

Aim of the paper is to introduce a new way for neural networks to learn and show that because it works well enough on a few small problems that it is worth further investigation. The goal of the FF is to replace the forward and backward passes of the traditional neural network with two forward passes, one with positive (real) data nd the other with negative data\footnote{What does negative data actually mean?} (generated by the network itself). Each layer has its own objective function which is to have high goodness\footnote{What is goodness?} for positive data and low goodness for negative data. 


\section{What is Wrong with Backpropagation}

\begin{itemize}
    \item Does the brain implement backpropagation?
    \item Seems like no - brain needs to pipeline sensory data through different stages of sensory processing and it needs a learning procedure that can learn on the fly
    \item Serious limitation of backprop is that it require perfect knowledge of the computation performed in forward pass\footnote{Need to expand on this}
    \begin{itemize}
        \item if we insert a black box into the forward pass can't perform backprop
        \item need to learn a differentiable model for the black box
        \item black box does not change the learning procedure for FF since no need to backpropagate
    \end{itemize}

    \item Main point of paper is to show that NN containing unknown non-linearities do not need to resort to reinforcement learning

\end{itemize}


\section{The Forward-Forward Algorithm}

\begin{itemize}
    \item Replace forward/backward passed of backpropagation with two forward passes that operate the exact same way
    \item positive pass operates on real data and adjusts weights to increase the goodness in every hidden layer
    \item negative pass operates on "negative data" and adjusts the weights to decrease goodness in every hidden layer
    \item two different measures of goodness:
    \begin{itemize}
        \item sum of the squared neural activities
        \item negative sum of the squared activities\footnote{what does negative sum mean?}
    \end{itemize}

    \item assume goodness function\footnote{Is goodness function == activation function?} for a layer is the sum of the squares of the activities of the rectified linear neurons in that layer
    \item aim of the learning is to make the goodness be well above some threshold value for real data and well below that value for the negative data

\end{itemize}

